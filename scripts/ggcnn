#!/usr/bin/env python
from __future__ import print_function

import rospy
import cv2
import cv_bridge
import tf2_ros
import tf2_geometry_msgs
from tf import transformations
import numpy as np


from cloudvis.msg import Property

from cloudvis.srv import Get, GetRequest
from rv_msgs.srv import GetGraspPose, GetGraspPoseResponse

from sensor_msgs.msg import Image, CameraInfo
from geometry_msgs.msg import Pose, PoseStamped, Quaternion

def process_depth_mask(depth, crop_size, out_size=300, crop_y_offset=0):
  imh, imw = depth.shape

  # Crop.
  depth_crop = depth[(imh - crop_size) // 2 - crop_y_offset:(imh - crop_size) // 2 + crop_size - crop_y_offset,
                      (imw - crop_size) // 2:(imw - crop_size) // 2 + crop_size]

  depth_crop = cv2.resize(depth_crop, (out_size, out_size), cv2.INTER_AREA)

  return depth_crop

def process_depth_image(depth, crop_size, out_size=300, return_mask=False, crop_y_offset=0):
    imh, imw = depth.shape

    # Crop.
    depth_crop = depth[(imh - crop_size) // 2 - crop_y_offset:(imh - crop_size) // 2 + crop_size - crop_y_offset,
                        (imw - crop_size) // 2:(imw - crop_size) // 2 + crop_size]

    # Inpaint
    # OpenCV inpainting does weird things at the border.
    depth_crop = cv2.copyMakeBorder(depth_crop, 1, 1, 1, 1, cv2.BORDER_DEFAULT)
    depth_nan_mask = np.isnan(depth_crop).astype(np.uint8)

    depth_crop[depth_nan_mask==1] = 0

    # Scale to keep as float, but has to be in bounds -1:1 to keep opencv happy.
    depth_scale = np.abs(depth_crop).max()
    depth_crop = depth_crop.astype(np.float32) / depth_scale  # Has to be float32, 64 not supported.

    depth_crop = cv2.inpaint(depth_crop, depth_nan_mask, 1, cv2.INPAINT_NS)

    # Back to original size and value range.
    depth_crop = depth_crop[1:-1, 1:-1]
    depth_crop = depth_crop * depth_scale

    # Resize
    depth_crop = cv2.resize(depth_crop, (out_size, out_size), cv2.INTER_AREA)

    if return_mask:
        depth_nan_mask = depth_nan_mask[1:-1, 1:-1]
        depth_nan_mask = cv2.resize(depth_nan_mask, (out_size, out_size), cv2.INTER_NEAREST)
        return depth_crop, depth_nan_mask
    else:
        return depth_crop

class CloudvisGGCNN(object):
  def __init__(self):
    self.service = rospy.Service('/service/ggcnn', GetGraspPose, self.request_cb)
    self.bridge = cv_bridge.CvBridge()

    self.base_frame = rospy.get_param('~camera/robot_base_frame')
    self.camera_frame = rospy.get_param('~camera/camera_frame')
    self.img_crop_size = rospy.get_param('~camera/crop_size')
    self.img_crop_y_offset = rospy.get_param('~camera/crop_y_offset')
    self.cam_fov = rospy.get_param('~camera/fov')


    self.tf_buffer = tf2_ros.Buffer()
    self.listener = tf2_ros.TransformListener(self.tf_buffer)
    
  def request_cb(self, request):
    proxy = rospy.ServiceProxy('cloudvis', Get)
    proxy.wait_for_service()

    cam_K = np.array(request.camera_info.K).reshape((3, 3))

    depth = self.bridge.imgmsg_to_cv2(request.depth, desired_encoding="passthrough")
    mask = self.bridge.imgmsg_to_cv2(request.mask, desired_encoding="passthrough")

    depth_crop, _ = process_depth_image(depth, self.img_crop_size, 300, return_mask=True, crop_y_offset=self.img_crop_y_offset)
    mask_crop = process_depth_mask(mask, self.img_crop_size, 300, crop_y_offset=self.img_crop_y_offset)
    
    camera_pose = self._current_robot_pose(self.base_frame, request.camera_info.header.frame_id)
    camera_rot = transformations.quaternion_matrix(self._quaternion_to_list(camera_pose.orientation))[0:3, 0:3]
    cam_p = camera_pose.position

    req = GetRequest()
    req.service_name = 'ggcnn'

    req.properties.append(Property(name='image', image=request.depth))

    response = proxy(req)

    points = self.bridge.imgmsg_to_cv2([result.image for result in response.result if result.name == 'image'][0])
    angle = self.bridge.imgmsg_to_cv2([result.image for result in response.result if result.name == 'angle'][0])
    width_img = self.bridge.imgmsg_to_cv2([result.image for result in response.result if result.name == 'width'][0])

    angle = angle.copy()

    quadrant = np.arctan2(camera_pose.position.y, camera_pose.position.y)

    angle -= np.arcsin(camera_rot[0, 1])  # Correct for the rotation of the camera
    angle = angle % np.pi # @TODO make this work across the entire unit circle

    # Convert to 3D positions.
    imh, imw = depth.shape
    x = ((np.vstack((np.linspace((imw - self.img_crop_size) // 2, (imw - self.img_crop_size) // 2 + self.img_crop_size, depth_crop.shape[1], np.float), )*depth_crop.shape[0]) - cam_K[0, 2])/cam_K[0, 0] * depth_crop).flatten()
    y = ((np.vstack((np.linspace((imh - self.img_crop_size) // 2 - self.img_crop_y_offset, (imh - self.img_crop_size) // 2 + self.img_crop_size - self.img_crop_y_offset, depth_crop.shape[0], np.float), )*depth_crop.shape[1]).T - cam_K[1,2])/cam_K[1, 1] * depth_crop).flatten()
    pos = np.dot(camera_rot, np.stack((x, y, depth_crop.flatten()))).T + np.array([[cam_p.x, cam_p.y, cam_p.z]])

    width_m = width_img / 300.0 * 2.0 * depth_crop * np.tan(self.cam_fov * self.img_crop_size/depth.shape[0] / 2.0 / 180.0 * np.pi)
    
    masked_data = cv2.bitwise_and(points, points, mask=mask_crop)
    # cv2.imshow('Mask', mask)
    # cv2.imshow('Mask Cropped', mask_crop)
    # cv2.imshow('Masked data', masked_data)
    # cv2.imshow('points', points)
    # cv2.waitKey(0)
    # cv2.destroyAllWindows()
    best_g = np.argmax(masked_data)
    best_g_unr = np.unravel_index(best_g, points.shape)

    result = GetGraspPoseResponse()
    result.grasp_pose.header.frame_id = self.base_frame
    result.grasp_pose.pose.position.x = pos[best_g, 0]
    result.grasp_pose.pose.position.y = pos[best_g, 1]
    result.grasp_pose.pose.position.z = pos[best_g, 2]
    result.grasp_pose.pose.orientation = self._list_to_quaternion(transformations.quaternion_from_euler(np.pi, 0, angle[best_g_unr] - np.pi/2))
    result.width = width_m[best_g_unr]
    result.quality = points[best_g_unr]

    return result

  def _quaternion_to_list(self, quaternion):
      return [quaternion.x, quaternion.y, quaternion.z, quaternion.w]

  def _list_to_quaternion(self, l):
    q = Quaternion()
    q.x = l[0]
    q.y = l[1]
    q.z = l[2]
    q.w = l[3]
    return q

  def _current_robot_pose(self, reference_frame, base_frame):
      """
      Get the current pose of the robot in the given reference frame
          reference_frame         -> A string that defines the reference_frame that the robots current pose will be defined in
      """
      # Create Pose
      p = Pose()
      p.orientation.w = 1.0

      # Transforms robots current pose to the base reference frame
      return self._convert_pose(p, base_frame, reference_frame)


  def _convert_pose(self, pose, from_frame, to_frame):
      """
      Convert a pose or transform between frames using tf.
          pose            -> A geometry_msgs.msg/Pose that defines the robots position and orientation in a reference_frame
          from_frame      -> A string that defines the original reference_frame of the robot
          to_frame        -> A string that defines the desired reference_frame of the robot to convert to
      """
      try:
          trans = self.tf_buffer.lookup_transform(to_frame, from_frame, rospy.Time(0), rospy.Duration(1.0))
      except (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException), e:
          rospy.logerr('FAILED TO GET TRANSFORM FROM %s to %s' % (to_frame, from_frame))
          return None

      spose = PoseStamped()
      spose.pose = pose
      spose.header.stamp = rospy.Time().now
      spose.header.frame_id = from_frame

      p2 = tf2_geometry_msgs.do_transform_pose(spose, trans)

      return p2.pose

  def run(self):
    rospy.spin()

if __name__ == '__main__':
  rospy.init_node('ggcnn')

  cloudvis = CloudvisGGCNN()
  cloudvis.run()
